{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZDaYfJ1BrjO"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","from torch.utils.data import Dataset"]},{"cell_type":"code","source":["!pip install opendatasets --quiet\n","import opendatasets as od\n","\n","od.download( \"https://www.kaggle.com/competitions/automated-video-captioning/data\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dl-fw1l5FBTL","outputId":"1ba216fd-feb8-4ea1-c572-c515b3d7127f","executionInfo":{"status":"ok","timestamp":1749577714373,"user_tz":-180,"elapsed":60155,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n","Your Kaggle username: anizuro\n","Your Kaggle Key: ··········\n","Downloading automated-video-captioning.zip to ./automated-video-captioning\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.08G/1.08G [00:08<00:00, 132MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Extracting archive ./automated-video-captioning/automated-video-captioning.zip to ./automated-video-captioning\n"]}]},{"cell_type":"code","source":["train = pd.read_csv(\"/content/automated-video-captioning/train.csv\")\n","test = pd.read_csv(\"/content/automated-video-captioning/test.csv\")\n","train"],"metadata":{"id":"SXfDYDFCFkhz","colab":{"base_uri":"https://localhost:8080/","height":424},"outputId":"e3783482-4215-48dc-f9df-5f6a71f5db18","executionInfo":{"status":"ok","timestamp":1749577714469,"user_tz":-180,"elapsed":84,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     index file_name                                            caption\n","0        0     0.mp4  A man is working out on a seated chest press m...\n","1        1     1.mp4  Preparing a bowl with yogurt and assorted fres...\n","2        2     2.mp4  A man with a muscular build is seen from behin...\n","3        3     3.mp4  Man exercising by jogging on a pedestrian brid...\n","4        4     4.mp4    Wristwatch hands moving forward close-up views.\n","..     ...       ...                                                ...\n","598    598   598.mp4  Terrifying Halloween pumpkin with a carved evi...\n","599    599   599.mp4  Cheerful girl sending messages on cell phone w...\n","600    600   600.mp4  A person in a light blue shirt is using a lapt...\n","601    601   601.mp4  A top-down view of an aesthetic desk setup, wi...\n","602    602   602.mp4  A young man with a sleek, black streaming micr...\n","\n","[603 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-5f0673f9-d6b1-4a40-ba75-65cd04029211\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>file_name</th>\n","      <th>caption</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.mp4</td>\n","      <td>A man is working out on a seated chest press m...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1.mp4</td>\n","      <td>Preparing a bowl with yogurt and assorted fres...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2.mp4</td>\n","      <td>A man with a muscular build is seen from behin...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3.mp4</td>\n","      <td>Man exercising by jogging on a pedestrian brid...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4.mp4</td>\n","      <td>Wristwatch hands moving forward close-up views.</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>598</th>\n","      <td>598</td>\n","      <td>598.mp4</td>\n","      <td>Terrifying Halloween pumpkin with a carved evi...</td>\n","    </tr>\n","    <tr>\n","      <th>599</th>\n","      <td>599</td>\n","      <td>599.mp4</td>\n","      <td>Cheerful girl sending messages on cell phone w...</td>\n","    </tr>\n","    <tr>\n","      <th>600</th>\n","      <td>600</td>\n","      <td>600.mp4</td>\n","      <td>A person in a light blue shirt is using a lapt...</td>\n","    </tr>\n","    <tr>\n","      <th>601</th>\n","      <td>601</td>\n","      <td>601.mp4</td>\n","      <td>A top-down view of an aesthetic desk setup, wi...</td>\n","    </tr>\n","    <tr>\n","      <th>602</th>\n","      <td>602</td>\n","      <td>602.mp4</td>\n","      <td>A young man with a sleek, black streaming micr...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>603 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f0673f9-d6b1-4a40-ba75-65cd04029211')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5f0673f9-d6b1-4a40-ba75-65cd04029211 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5f0673f9-d6b1-4a40-ba75-65cd04029211');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-424448b8-a17c-4929-8c24-3cc74b98b5ae\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-424448b8-a17c-4929-8c24-3cc74b98b5ae')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-424448b8-a17c-4929-8c24-3cc74b98b5ae button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train","summary":"{\n  \"name\": \"train\",\n  \"rows\": 603,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 174,\n        \"min\": 0,\n        \"max\": 602,\n        \"num_unique_values\": 603,\n        \"samples\": [\n          110,\n          527,\n          567\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 603,\n        \"samples\": [\n          \"110.mp4\",\n          \"527.mp4\",\n          \"567.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 603,\n        \"samples\": [\n          \"Young blond-haired woman lying on a sofa, takes a book from the coffee table in her living room, and begins to read it concentratedly.\",\n          \"Face of a man concentrating on his Smartphone while being surprised by his girlfriend, who gives him an affectionate kiss on the cheek.\",\n          \"A couple poses for a photo, with the woman in front and the man\\u2019s arms wrapped around her, capturing a loving moment. The green screen backdrop offers customization. Perfect for themes of love, happiness, and togetherness in ads or social content.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('wordnet')\n","\n","MAX_LENGTH = 50\n","PAD_TOKEN = '<PAD>'\n","SOS_TOKEN = '<SOS>'\n","EOS_TOKEN = '<EOS>'\n","OOV_TOKEN = '<UNK>'\n","\n","def tokenize_and_lemmatize(text):\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = word_tokenize(str(text).lower())\n","    return [lemmatizer.lemmatize(token) for token in tokens]\n","\n","train['tokens'] = train['caption'].apply(\n","    lambda x: [SOS_TOKEN] + tokenize_and_lemmatize(x) + [EOS_TOKEN]\n",")\n","train['text_for_keras'] = train['tokens'].apply(' '.join)\n","\n","tokenizer = Tokenizer(\n","    num_words=None,\n","    oov_token=OOV_TOKEN,\n","    filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",")\n","tokenizer.fit_on_texts(train['text_for_keras'])\n","\n","if PAD_TOKEN not in tokenizer.word_index:\n","    tokenizer.word_index[PAD_TOKEN] = len(tokenizer.word_index) + 1\n","if SOS_TOKEN not in tokenizer.word_index:\n","    tokenizer.word_index[SOS_TOKEN] = len(tokenizer.word_index) + 1\n","if EOS_TOKEN not in tokenizer.word_index:\n","    tokenizer.word_index[EOS_TOKEN] = len(tokenizer.word_index) + 1\n","\n","tokenizer.index_word = {v: k for k, v in tokenizer.word_index.items()}\n","\n","sequences = tokenizer.texts_to_sequences(train['text_for_keras'])\n","\n","padded_sequences = pad_sequences(\n","    sequences,\n","    maxlen=MAX_LENGTH,\n","    padding='post',\n","    truncating='post',\n","    value=tokenizer.word_index[PAD_TOKEN]\n",")\n","\n","train['token_ids'] = list(padded_sequences)\n","train = train.drop(columns=['caption', 'text_for_keras'])\n","\n","print(\"Результат (первые 5 строк):\")\n","print(train[['tokens', 'token_ids']].head())\n","\n","print(\"\\nСловарь (ключевые токены):\")\n","special_tokens = {\n","    PAD_TOKEN: tokenizer.word_index[PAD_TOKEN],\n","    SOS_TOKEN: tokenizer.word_index[SOS_TOKEN],\n","    EOS_TOKEN: tokenizer.word_index[EOS_TOKEN],\n","    OOV_TOKEN: tokenizer.word_index[OOV_TOKEN]\n","}\n","print(special_tokens)"],"metadata":{"id":"9oER5P7subWp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9cca0a62-47c3-425e-e873-6cbe0e651661","executionInfo":{"status":"ok","timestamp":1749577736941,"user_tz":-180,"elapsed":22461,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Результат (первые 5 строк):\n","                                              tokens  \\\n","0  [<SOS>, a, man, is, working, out, on, a, seate...   \n","1  [<SOS>, preparing, a, bowl, with, yogurt, and,...   \n","2  [<SOS>, a, man, with, a, muscular, build, is, ...   \n","3  [<SOS>, man, exercising, by, jogging, on, a, p...   \n","4  [<SOS>, wristwatch, hand, moving, forward, clo...   \n","\n","                                           token_ids  \n","0  [3, 2, 16, 24, 102, 54, 10, 2, 320, 432, 204, ...  \n","1  [3, 205, 2, 321, 8, 371, 7, 794, 1066, 1067, 8...  \n","2  [3, 2, 16, 8, 2, 90, 434, 24, 112, 26, 113, 24...  \n","3  [3, 16, 104, 27, 127, 10, 2, 323, 186, 6, 5, 6...  \n","4  [3, 1068, 23, 249, 518, 33, 30, 36, 4, 1943, 1...  \n","\n","Словарь (ключевые токены):\n","{'<PAD>': 1943, '<SOS>': 1944, '<EOS>': 1945, '<UNK>': 1}\n"]}]},{"cell_type":"code","source":["class VideoCaptionDataset(Dataset):\n","    def __init__(self, dataframe, video_dir, transform=None, num_frames=16):\n","        self.dataframe = dataframe\n","        self.video_dir = video_dir\n","        self.transform = transform\n","        self.num_frames = num_frames\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        video_path = os.path.join(self.video_dir, self.dataframe.iloc[idx]['file_name'])\n","        token_ids = self.dataframe.iloc[idx]['token_ids']\n","\n","        frames = self.extract_frames(video_path)\n","\n","        if self.transform:\n","            frames = [self.transform(frame) for frame in frames]\n","\n","        frames = torch.stack(frames)\n","\n","        src_tokens = token_ids[:-1]\n","        tgt_tokens = token_ids[:-1]\n","        targets = token_ids[1:]\n","\n","        return {\n","            'frames': frames,\n","            'src_tokens': torch.tensor(src_tokens, dtype=torch.long),\n","            'tgt_tokens': torch.tensor(tgt_tokens, dtype=torch.long),\n","            'targets': torch.tensor(targets, dtype=torch.long),\n","            'video_path': video_path\n","        }\n","\n","    def extract_frames(self, video_path):\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        frame_indices = np.linspace(0, total_frames-1, self.num_frames, dtype=int)\n","\n","        for idx in frame_indices:\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","            ret, frame = cap.read()\n","            if ret:\n","                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                frames.append(frame)\n","\n","        cap.release()\n","        return frames"],"metadata":{"id":"sL0jiiK-LsJL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VideoTestDataset(Dataset):\n","    def __init__(self, dataframe, video_dir, transform=None, num_frames=16):\n","        self.dataframe = dataframe\n","        self.video_dir = video_dir\n","        self.transform = transform\n","        self.num_frames = num_frames\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        video_path = os.path.join(self.video_dir, self.dataframe.iloc[idx]['file_name'])\n","        frames = self.extract_frames(video_path)\n","\n","        if self.transform:\n","            frames = [self.transform(frame) for frame in frames]\n","\n","        frames = torch.stack(frames)\n","\n","        return {\n","            'frames': frames,\n","            'video_path': video_path\n","        }\n","\n","    def extract_frames(self, video_path):\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        frame_indices = np.linspace(0, total_frames-1, self.num_frames, dtype=int)\n","\n","        for idx in frame_indices:\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","            ret, frame = cap.read()\n","            if ret:\n","                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                frames.append(frame)\n","\n","        cap.release()\n","        return frames"],"metadata":{"id":"qd9AUlFvxQ7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"lv3-ooh-Ob0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","def collate_fn(batch):\n","    try:\n","        frames = torch.stack([item['frames'] for item in batch])\n","        src_tokens = torch.stack([item['src_tokens'] for item in batch])\n","        tgt_tokens = torch.stack([item['tgt_tokens'] for item in batch])\n","        targets = torch.stack([item['targets'] for item in batch])\n","        video_paths = [item['video_path'] for item in batch]\n","\n","        return {\n","            'frames': frames,\n","            'src_tokens': src_tokens,\n","            'tgt_tokens': tgt_tokens,\n","            'targets': targets,\n","            'video_paths': video_paths\n","        }\n","    except Exception as e:\n","        print(\"Ошибка при создании батча:\", e)\n","        raise\n","\n","dataset = VideoCaptionDataset(\n","    dataframe=train,\n","    video_dir='/content/automated-video-captioning/train_videos',\n","    transform=transform,\n","    num_frames=16\n",")\n","\n","dataloader = DataLoader(\n","    dataset,\n","    batch_size=4,\n","    shuffle=True,\n","    num_workers=2,\n","    collate_fn=collate_fn\n",")"],"metadata":{"id":"H8utm3FMPBlw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn_test(batch):\n","    try:\n","        frames = torch.stack([item['frames'] for item in batch])\n","        video_paths = [item['video_path'] for item in batch]\n","\n","        return {\n","            'frames': frames,\n","            'video_paths': video_paths\n","        }\n","    except Exception as e:\n","        print(\"Ошибка при создании батча (test):\", e)\n","        raise\n","\n","test_dataset = VideoTestDataset(\n","    dataframe=test,\n","    video_dir='/content/automated-video-captioning/test_videos',\n","    transform=transform,\n","    num_frames=16\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=4,\n","    shuffle=False,\n","    num_workers=2,\n","    collate_fn=collate_fn_test\n",")"],"metadata":{"id":"DobOCFc8xZiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Проверка данных:\")\n","print(\"Тип token_ids:\", type(train['token_ids'].iloc[0]))\n","print(\"Длина token_ids:\", len(train['token_ids'].iloc[0]))\n","print(\"Пример token_ids:\", train['token_ids'].iloc[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9OuGEaGsaaKY","outputId":"9791b0fe-71d1-47ef-b09a-b54b4bc0a614","executionInfo":{"status":"ok","timestamp":1749577746187,"user_tz":-180,"elapsed":14,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Проверка данных:\n","Тип token_ids: <class 'numpy.ndarray'>\n","Длина token_ids: 50\n","Пример token_ids: [   3    2   16   24  102   54   10    2  320  432  204   88    6    2\n","   80   17   85   24   52    7   48    2   35  793  247    7  370    5\n","   17  433  222  514  184    7  149   59    6    5   21    4 1943 1943\n"," 1943 1943 1943 1943 1943 1943 1943 1943]\n"]}]},{"cell_type":"code","source":["for batch in dataloader:\n","    print(\"Размеры батча:\")\n","    print(\"Кадры:\", batch['frames'].shape)\n","    print(\"src_tokens:\", batch['src_tokens'].shape)\n","    print(\"tgt_tokens:\", batch['tgt_tokens'].shape)\n","    print(\"targets:\", batch['targets'].shape)\n","    print(\"Пример src_tokens:\", [tokenizer.index_word[i.item()] for i in batch['src_tokens'][0]])\n","    print(\"Пример targets:\", [tokenizer.index_word[i.item()] for i in batch['targets'][0]])\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3etN0SOaOj8","outputId":"750219cf-1624-4175-9ca9-39524012297b","executionInfo":{"status":"ok","timestamp":1749577766683,"user_tz":-180,"elapsed":20493,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Размеры батча:\n","Кадры: torch.Size([4, 16, 3, 224, 224])\n","src_tokens: torch.Size([4, 49])\n","tgt_tokens: torch.Size([4, 49])\n","targets: torch.Size([4, 49])\n","Пример src_tokens: ['<sos>', 'a', 'young', 'shirtless', 'man', 'work', 'out', 'on', 'the', 'elliptical', 'machine', 'his', 'gaze', 'fixed', 'ahead', 'with', 'unwavering', 'determination', 'his', 'movement', 'are', 'steady', 'and', 'rhythmic', 'each', 'stride', 'a', 'testament', 'to', 'his', 'commitment', 'to', 'cardiovascular', 'fitness', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","Пример targets: ['a', 'young', 'shirtless', 'man', 'work', 'out', 'on', 'the', 'elliptical', 'machine', 'his', 'gaze', 'fixed', 'ahead', 'with', 'unwavering', 'determination', 'his', 'movement', 'are', 'steady', 'and', 'rhythmic', 'each', 'stride', 'a', 'testament', 'to', 'his', 'commitment', 'to', 'cardiovascular', 'fitness', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"]}]},{"cell_type":"markdown","source":["Модель"],"metadata":{"id":"rVg9iEBugskJ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import resnet50\n","\n","class Bumblebee(nn.Module):\n","    def __init__(self, vocab_size, d_model=512, lstm_hidden=512, lstm_layers=2, dropout=0.1, max_len=100):\n","        super().__init__()\n","        resnet = resnet50(pretrained=True)\n","      modules = list(resnet.children())[:-1]\n","        self.cnn = nn.Sequential(*modules)\n","        self.cnn_out_dim = 2048\n","\n","        self.visual_projection = nn.Linear(self.cnn_out_dim, d_model)\n","\n","        self.token_embedding = nn.Embedding(vocab_size, d_model)\n","        self.positional_encoding = PositionalEncoding(d_model, dropout, max_len)\n","\n","        self.lstm_encoder = nn.LSTM(\n","            input_size=d_model, hidden_size=lstm_hidden, num_layers=lstm_layers,\n","            batch_first=True, dropout=dropout, bidirectional=False\n","        )\n","\n","        self.lstm_decoder = nn.LSTM(\n","            input_size=d_model, hidden_size=lstm_hidden, num_layers=lstm_layers,\n","            batch_first=True, dropout=dropout, bidirectional=False\n","        )\n","\n","        self.output_layer = nn.Linear(lstm_hidden, vocab_size)\n","\n","        self.max_len = max_len\n","        self.d_model = d_model\n","        self.lstm_hidden = lstm_hidden\n","        self.lstm_layers = lstm_layers\n","\n","    def forward(self, frames, src_tokens, tgt_tokens):\n","        B, T, C, H, W = frames.shape\n","\n","        frames = frames.view(B * T, C, H, W)\n","        with torch.no_grad():\n","            cnn_feats = self.cnn(frames).squeeze(-1).squeeze(-1)\n","        cnn_feats = cnn_feats.view(B, T, self.cnn_out_dim)\n","\n","        video_feat = cnn_feats.mean(dim=1)\n","        video_feat = self.visual_projection(video_feat).unsqueeze(1)\n","\n","        src_emb = self.token_embedding(src_tokens)\n","        src_emb = self.positional_encoding(src_emb)\n","\n","        tgt_emb = self.token_embedding(tgt_tokens)\n","        tgt_emb = self.positional_encoding(tgt_emb)\n","\n","        encoder_input = torch.cat([video_feat, src_emb], dim=1)\n","\n","        encoder_outputs, (h_n, c_n) = self.lstm_encoder(encoder_input)\n","\n","        decoder_outputs, _ = self.lstm_decoder(tgt_emb, (h_n, c_n))\n","\n","        logits = self.output_layer(decoder_outputs)\n","        return logits\n","\n","    def generate(self, frames, src_tokens, start_token_id, end_token_id, max_length=20, temperature=1.0):\n","        self.eval()\n","        B, T, C, H, W = frames.shape\n","        device = frames.device\n","\n","        with torch.no_grad():\n","            frames_reshaped = frames.view(B * T, C, H, W)\n","            cnn_feats = self.cnn(frames_reshaped).squeeze(-1).squeeze(-1)\n","            cnn_feats = cnn_feats.view(B, T, self.cnn_out_dim)\n","            video_feat = cnn_feats.mean(dim=1)\n","            video_feat = self.visual_projection(video_feat).unsqueeze(1)\n","\n","            src_emb = self.token_embedding(src_tokens)\n","            src_emb = self.positional_encoding(src_emb)\n","\n","            encoder_input = torch.cat([video_feat, src_emb], dim=1)\n","            encoder_outputs, (h_n, c_n) = self.lstm_encoder(encoder_input)\n","\n","            generated = torch.full((B, 1), start_token_id, dtype=torch.long, device=device)\n","            hidden = (h_n, c_n)\n","\n","            finished = torch.zeros(B, dtype=torch.bool, device=device)\n","\n","            for _ in range(max_length):\n","                tgt_emb = self.token_embedding(generated[:, -1:])\n","                tgt_emb = self.positional_encoding(tgt_emb)\n","\n","                output, hidden = self.lstm_decoder(tgt_emb, hidden)\n","                logits = self.output_layer(output.squeeze(1))\n","                probs = F.softmax(logits / temperature, dim=-1)\n","\n","                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n","                next_tokens = next_tokens.masked_fill(finished, end_token_id)\n","\n","                generated = torch.cat([generated, next_tokens.unsqueeze(1)], dim=1)\n","                finished |= next_tokens == end_token_id\n","\n","                if finished.all():\n","                    break\n","\n","            return [seq.tolist()[1:] for seq in generated]\n","\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=500):\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return self.dropout(x)"],"metadata":{"id":"u_vw2mYRhKLQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from tqdm import tqdm\n","from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n","from tqdm import tqdm\n","\n","nltk.download('punkt')\n","\n","def decode_tokens(token_ids, tokenizer):\n","    if isinstance(token_ids, torch.Tensor):\n","        token_ids = token_ids.cpu().tolist()\n","\n","    words = []\n","    for idx in token_ids:\n","        if idx == tokenizer.word_index['<PAD>']:\n","            continue\n","        word = tokenizer.index_word.get(idx, '<unk>')\n","        if word in ['<PAD>', '<SOS>', '<EOS>']:\n","            continue\n","        words.append(word)\n","    return words\n","\n","bleu_N = 10000\n","\n","def calculate_bleu(model, dataloader, tokenizer, device):\n","    print('Calculating BLEU...', end=' ')\n","\n","    model.eval()\n","    references = []\n","    hypotheses = []\n","    smoothing = SmoothingFunction().method4\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(tqdm(dataloader)):\n","            if i > bleu_N:\n","              break\n","\n","            frames = batch['frames'].to(device)\n","            src_tokens = batch['src_tokens'].to(device)\n","            tgt_tokens = batch['tgt_tokens'].to(device)\n","            targets = batch['targets'].to(device)\n","\n","            preds = model.generate(frames, src_tokens, tokenizer.word_index['<SOS>'], tokenizer.word_index['<EOS>'])\n","\n","            for i in range(targets.size(0)):\n","                ref = decode_tokens(targets[i], tokenizer)\n","                hyp = decode_tokens(preds[i], tokenizer)\n","\n","                if len(hyp) == 0:\n","                    hyp = ['<unk>']\n","\n","                references.append([ref])\n","                hypotheses.append(hyp)\n","\n","    bleu_score = corpus_bleu(\n","        references,\n","        hypotheses,\n","        smoothing_function=smoothing,\n","        weights=(0.25, 0.25, 0.25, 0.25)\n","    )\n","    return bleu_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOQlXq4dF6NN","outputId":"d77a4229-e837-4e30-c6d6-270bb1d118e5","executionInfo":{"status":"ok","timestamp":1749577766705,"user_tz":-180,"elapsed":31,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","vocab_size = len(tokenizer.word_index) + 1\n","model = Bumblebee(vocab_size=vocab_size).to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","EPOCHS = 10\n","\n","pad_idx = tokenizer.word_index['<PAD>']\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"],"metadata":{"id":"f-LznkFfF-H_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749577768301,"user_tz":-180,"elapsed":1578,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}},"outputId":"3eabc9d8-7583-4716-9c06-249f02de33a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 193MB/s]\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AAUP-0wMt7e_","outputId":"137ae3c8-6675-440a-84bf-98c89e13b839","executionInfo":{"status":"ok","timestamp":1749578065680,"user_tz":-180,"elapsed":8661,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["for epoch in range(EPOCHS):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(dataloader):\n","        frames = batch['frames'].to(device)\n","        src_tokens = batch['src_tokens'].to(device)\n","        tgt_tokens = batch['tgt_tokens'].to(device)\n","        targets = batch['targets'].to(device)\n","\n","        outputs = model(frames, src_tokens, tgt_tokens)\n","        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    torch.save(model.state_dict(), f'/content/drive/MyDrive/model_weights_epoch_{epoch}.pth')\n","    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"ZnD4T7LYt6Z7","outputId":"19931064-1be2-436e-ed15-5534ffd2db2d","executionInfo":{"status":"error","timestamp":1749582090382,"user_tz":-180,"elapsed":4022346,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 151/151 [10:58<00:00,  4.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 6.1607\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 151/151 [11:03<00:00,  4.39s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Loss: 5.6171\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 151/151 [11:00<00:00,  4.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Loss: 5.4490\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 151/151 [11:00<00:00,  4.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Loss: 5.3005\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 151/151 [10:56<00:00,  4.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Loss: 5.1379\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 151/151 [10:57<00:00,  4.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, Loss: 4.9914\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▊         | 13/151 [01:02<11:03,  4.81s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-613485972>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'model_weights_2.pth')"],"metadata":{"id":"hvtOZmG0hNhc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Генерация ответа"],"metadata":{"id":"PXfwQhG2yRKe"}},{"cell_type":"code","source":["model.eval()\n","\n","start_token_id, end_token_id = tokenizer.word_index['<SOS>'], tokenizer.word_index['<EOS>']\n","pad_token_id = tokenizer.word_index['<PAD>']\n","\n","results = []\n","index = 0\n","\n","for batch in tqdm(test_dataloader):\n","    frames = batch['frames'].to(device)\n","    video_paths = batch['video_paths']\n","\n","    B = frames.size(0)\n","    src_tokens = torch.full((B, 1), start_token_id, dtype=torch.long, device=device)\n","\n","    generated_ids = model.generate(\n","        frames=frames,\n","        src_tokens=src_tokens,\n","        start_token_id=start_token_id,\n","        end_token_id=end_token_id,\n","        max_length=30\n","    )\n","    for video_path, token_ids in zip(video_paths, generated_ids):\n","        caption = decode_tokens(token_ids, tokenizer)\n","        file_name = os.path.basename(video_path)\n","        results.append((index, file_name, caption))\n","        index += 1\n","\n","df = pd.DataFrame(results, columns=[\"index\", \"file_name\", \"caption\"])\n","df.to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"F7oaePWOg2EE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"11a894c9-773e-4ac1-b712-11d2a1df1062","executionInfo":{"status":"ok","timestamp":1749582635723,"user_tz":-180,"elapsed":539212,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 131/131 [08:59<00:00,  4.12s/it]\n"]}]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"sYL1Eu-_yzJc","outputId":"d8a32074-62d2-4576-f6cc-022cd2f9896f","executionInfo":{"status":"ok","timestamp":1749582739250,"user_tz":-180,"elapsed":37,"user":{"displayName":"Арэс Ткаченко","userId":"17013061520904249666"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     index file_name                                            caption\n","0        0     0.mp4  [skyline, highlight, entire, lip, <eos>, <eos>...\n","1        1     1.mp4  [sunny, give, companion, she, for, <eos>, jade...\n","2        2     2.mp4  [valentine, enhancing, during, <eos>, <eos>, f...\n","3        3     3.mp4  [cinematic, powerfully, weight, colorful, brid...\n","4        4     4.mp4  [magnificent, cell, underwear, summer, banana,...\n","..     ...       ...                                                ...\n","516    516   516.mp4  [sweat, smoothly, desk, lift, with, a, <eos>, ...\n","517    517   517.mp4  [sprint, los, fluently, training, while, intwi...\n","518    518   518.mp4  [amidst, start, through, afternoon, background...\n","519    519   519.mp4  [with, reporter, stable, while, ups, <eos>, <e...\n","520    520   520.mp4  [rubbing, surrounded, dio, offer, face, and, w...\n","\n","[521 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-da0d2b0f-a9cd-4210-acab-800d56828e6f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>file_name</th>\n","      <th>caption</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.mp4</td>\n","      <td>[skyline, highlight, entire, lip, &lt;eos&gt;, &lt;eos&gt;...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1.mp4</td>\n","      <td>[sunny, give, companion, she, for, &lt;eos&gt;, jade...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2.mp4</td>\n","      <td>[valentine, enhancing, during, &lt;eos&gt;, &lt;eos&gt;, f...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3.mp4</td>\n","      <td>[cinematic, powerfully, weight, colorful, brid...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4.mp4</td>\n","      <td>[magnificent, cell, underwear, summer, banana,...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>516</th>\n","      <td>516</td>\n","      <td>516.mp4</td>\n","      <td>[sweat, smoothly, desk, lift, with, a, &lt;eos&gt;, ...</td>\n","    </tr>\n","    <tr>\n","      <th>517</th>\n","      <td>517</td>\n","      <td>517.mp4</td>\n","      <td>[sprint, los, fluently, training, while, intwi...</td>\n","    </tr>\n","    <tr>\n","      <th>518</th>\n","      <td>518</td>\n","      <td>518.mp4</td>\n","      <td>[amidst, start, through, afternoon, background...</td>\n","    </tr>\n","    <tr>\n","      <th>519</th>\n","      <td>519</td>\n","      <td>519.mp4</td>\n","      <td>[with, reporter, stable, while, ups, &lt;eos&gt;, &lt;e...</td>\n","    </tr>\n","    <tr>\n","      <th>520</th>\n","      <td>520</td>\n","      <td>520.mp4</td>\n","      <td>[rubbing, surrounded, dio, offer, face, and, w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>521 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da0d2b0f-a9cd-4210-acab-800d56828e6f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-da0d2b0f-a9cd-4210-acab-800d56828e6f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-da0d2b0f-a9cd-4210-acab-800d56828e6f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-f8e12d1f-7ef1-4478-8399-58588abd46d7\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8e12d1f-7ef1-4478-8399-58588abd46d7')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-f8e12d1f-7ef1-4478-8399-58588abd46d7 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 521,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 150,\n        \"min\": 0,\n        \"max\": 520,\n        \"num_unique_values\": 521,\n        \"samples\": [\n          507,\n          93,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 521,\n        \"samples\": [\n          \"507.mp4\",\n          \"93.mp4\",\n          \"6.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["df['caption'] = df['caption'].apply(lambda x: ' '.join(x))"],"metadata":{"id":"6gyXpTQQ24M-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"nrPaC_oB3QVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zySgIH4n3n5x"},"execution_count":null,"outputs":[]}]}